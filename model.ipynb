{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.optimize import minimize\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 2.0.2\n",
      "Scipy version: 1.13.1\n",
      "Scikit-learn version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Scipy version:\", scipy.__version__)\n",
    "print(\"Scikit-learn version:\", sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not get exchangeTimezoneName for ticker 'ETR' reason: 'chart'\n",
      "$ETR: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ETR: No data found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$HWM: possibly delisted; no price data found  (1d 2018-01-01 -> 2025-02-09)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping HWM: No data found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$J: possibly delisted; no price data found  (1d 2018-01-01 -> 2025-02-09)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping J: No data found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$LEN: possibly delisted; no price data found  (1d 2018-01-01 -> 2025-02-09)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LEN: No data found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  499 of 499 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'Adj Close' column not found in data. Returning raw data.\n",
      "Price           Close                                                   \\\n",
      "Ticker              A       AAPL       ABBV ABNB        ABT       ACGL   \n",
      "Date                                                                     \n",
      "2018-01-02  64.180229  40.524342  71.669937  NaN  51.776566  27.988113   \n",
      "2018-01-03  65.813217  40.517292  72.791458  NaN  51.891056  28.013470   \n",
      "2018-01-04  65.319550  40.705490  72.376343  NaN  51.802982  28.118069   \n",
      "2018-01-05  66.363876  41.168938  73.636269  NaN  51.952717  28.007132   \n",
      "2018-01-08  66.506332  41.016026  72.456459  NaN  51.802982  28.010302   \n",
      "\n",
      "Price                                                     ...  Volume  \\\n",
      "Ticker             ACN        ADBE        ADI        ADM  ...     WTW   \n",
      "Date                                                      ...           \n",
      "2018-01-02  138.659622  177.699997  79.129036  32.879116  ...  817700   \n",
      "2018-01-03  139.299576  181.039993  80.110710  32.624870  ...  771200   \n",
      "2018-01-04  140.948990  183.220001  80.023064  33.174362  ...  800900   \n",
      "2018-01-05  142.111755  185.339996  80.347374  32.952927  ...  646700   \n",
      "2018-01-08  143.247330  185.039993  80.487595  32.879116  ...  632700   \n",
      "\n",
      "Price                                                                      \\\n",
      "Ticker           WY     WYNN      XEL       XOM     XYL      YUM      ZBH   \n",
      "Date                                                                        \n",
      "2018-01-02  3144000  2581200  2443400  11469300  877800  1747800  1818259   \n",
      "2018-01-03  2922800  3530900  4114900  13957700  778800  2554900  1368664   \n",
      "2018-01-04  2727800  2708800  2807000  10863000  796500  1971200  1105396   \n",
      "2018-01-05  2924800  2186600  3728000  11047600  666800  1927100  1095302   \n",
      "2018-01-08  2760800  1803800  3837600  10927100  554700  1599400  1286985   \n",
      "\n",
      "Price                        \n",
      "Ticker        ZBRA      ZTS  \n",
      "Date                         \n",
      "2018-01-02  310600  2135600  \n",
      "2018-01-03  253000  2328200  \n",
      "2018-01-04  435200  2534000  \n",
      "2018-01-05  301800  2166100  \n",
      "2018-01-08  218600  3631400  \n",
      "\n",
      "[5 rows x 2495 columns]\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 1: Data Collection (Live Data Integration)\n",
    "# ==========================\n",
    "\n",
    "# def fetch_sp500_tickers():\n",
    "#     url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "#     sp500_table = pd.read_html(url)[0]\n",
    "#     return sp500_table[['Symbol', 'GICS Sector']]\n",
    "\n",
    "# def fetch_stock_data(tickers, start_date, end_date):\n",
    "#     data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "#     return data.dropna(axis=1)\n",
    "\n",
    "# sp500_companies = fetch_sp500_tickers()\n",
    "# tickers = sp500_companies['Symbol'].tolist()\n",
    "\n",
    "# start_date = \"2018-01-01\"\n",
    "# end_date = datetime.today().strftime('%Y-%m-%d')  # Get latest available data\n",
    "# data = fetch_stock_data(tickers, start_date, end_date)\n",
    "\n",
    "\n",
    "# from datetime import datetime\n",
    "# import yfinance as yf\n",
    "# import pandas as pd\n",
    "\n",
    "# import pandas as pd\n",
    "# import yfinance as yf\n",
    "# from datetime import datetime\n",
    "\n",
    "def fetch_sp500_tickers():\n",
    "    \"\"\"Fetch S&P 500 tickers from Wikipedia.\"\"\"\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    sp500_table = pd.read_html(url)[0]\n",
    "    return sp500_table[['Symbol', 'GICS Sector']]\n",
    "\n",
    "def clean_tickers(tickers):\n",
    "    \"\"\"Fix tickers with dot notation for Yahoo Finance.\"\"\"\n",
    "    return [t.replace('.', '-') for t in tickers]  # BRK.B â†’ BRK-B\n",
    "\n",
    "def filter_valid_tickers(tickers, start_date, end_date):\n",
    "    \"\"\"Check which tickers have valid data on Yahoo Finance.\"\"\"\n",
    "    valid_tickers = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            data = yf.Ticker(ticker).history(start=start_date, end=end_date)\n",
    "            if not data.empty:\n",
    "                valid_tickers.append(ticker)\n",
    "            else:\n",
    "                print(f\"Skipping {ticker}: No data found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {ticker}: {e}\")\n",
    "    return valid_tickers\n",
    "\n",
    "def fetch_stock_data(tickers, start_date, end_date):\n",
    "    \"\"\"Download stock data for valid tickers.\"\"\"\n",
    "    data = yf.download(tickers, start=start_date, end=end_date)\n",
    "    \n",
    "    # Check if 'Adj Close' exists in the returned data\n",
    "    if 'Adj Close' in data:\n",
    "        return data['Adj Close'].dropna(axis=1)  # Remove columns with NaN\n",
    "    else:\n",
    "        print(\"Warning: 'Adj Close' column not found in data. Returning raw data.\")\n",
    "        return data  # Return the whole DataFrame for debugging\n",
    "\n",
    "# Fetch tickers and clean them\n",
    "sp500_companies = fetch_sp500_tickers()\n",
    "tickers = clean_tickers(sp500_companies['Symbol'].tolist())\n",
    "\n",
    "# Define date range\n",
    "start_date = \"2018-01-01\"\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Validate tickers and fetch data\n",
    "valid_tickers = filter_valid_tickers(tickers, start_date, end_date)\n",
    "data = fetch_stock_data(valid_tickers, start_date, end_date)\n",
    "\n",
    "# Display the first few rows\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhavy\\AppData\\Local\\Temp\\ipykernel_36620\\392514591.py:6: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  returns = data.pct_change().dropna()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              mean_return  volatility  cumulative_return  sharpe_ratio  \\\n",
      "Price Ticker                                                             \n",
      "Close A         -0.461323   -0.489201          -0.067621     -0.047251   \n",
      "      AAPL      -0.488133   -0.499602           1.201692     -0.772085   \n",
      "      ABBV      -0.457299   -0.487787           0.082550     -0.036496   \n",
      "      ABT       -0.443102   -0.508513           0.019517      0.721704   \n",
      "      ACGL      -0.487660   -0.486642           0.340338     -0.767860   \n",
      "\n",
      "              sortino_ratio  max_drawdown    var_95      beta  liquidity  \n",
      "Price Ticker                                                              \n",
      "Close A           -0.229715      0.805469  0.505174 -0.469136  -0.115417  \n",
      "      AAPL        -0.741814      0.993460  0.484087 -0.464818  -0.115415  \n",
      "      ABBV        -0.421039      0.700538  0.526798 -0.467043  -0.115417  \n",
      "      ABT          0.235046      1.200175  0.542720 -0.476886  -0.115418  \n",
      "      ACGL        -0.737378      0.310649  0.492774 -0.461966  -0.115422  \n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# ==========================\n",
    "# Step 2: Feature Engineering (Risk, Liquidity & Macro)\n",
    "# ==========================\n",
    "# Compute returns and volatility\n",
    "returns = data.pct_change().dropna()\n",
    "volatility = returns.rolling(window=20).std().dropna()\n",
    "\n",
    "def compute_beta(stock_returns, market_returns):\n",
    "    \"\"\"Calculate Beta for each stock.\"\"\"\n",
    "    if stock_returns.isna().sum() > 0 or market_returns.isna().sum() > 0:\n",
    "        return np.nan  # Handle missing values\n",
    "    \n",
    "    covariance = np.cov(stock_returns.dropna(), market_returns.dropna())[0, 1]\n",
    "    market_variance = np.var(market_returns.dropna())\n",
    "\n",
    "    return covariance / market_variance if market_variance > 0 else np.nan\n",
    "\n",
    "# Approximate S&P 500 market return\n",
    "sp500_returns = returns.mean(axis=1)\n",
    "\n",
    "# Compute feature matrix\n",
    "features = pd.DataFrame({\n",
    "    \"mean_return\": returns.mean() * 252,\n",
    "    \"volatility\": volatility.mean(),\n",
    "    \"cumulative_return\": (data.iloc[-1] / data.iloc[0]) - 1,\n",
    "    \"sharpe_ratio\": returns.mean() / (returns.std() + 1e-8),  # Avoid div-by-zero\n",
    "    \"sortino_ratio\": returns.mean() / (returns[returns < 0].std() + 1e-8),  # Avoid div-by-zero\n",
    "    \"max_drawdown\": (data / data.cummax() - 1).min(),\n",
    "    \"var_95\": returns.quantile(0.05),\n",
    "    \"beta\": [compute_beta(returns[ticker], sp500_returns) for ticker in returns.columns],\n",
    "    \"liquidity\": data.mean(),  # Approximate by average price\n",
    "})\n",
    "\n",
    "# Handle Inf and NaNs before scaling\n",
    "features.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace Inf with NaN\n",
    "features.dropna(inplace=True)  # Drop rows with NaN values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "features_scaled_df = pd.DataFrame(scaled_features, index=features.index, columns=features.columns)\n",
    "\n",
    "# Display first few rows\n",
    "print(features_scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              mean_return  volatility  cumulative_return  sharpe_ratio  \\\n",
      "Price Ticker                                                             \n",
      "Close A         -0.461323   -0.489201          -0.067621     -0.047251   \n",
      "      AAPL      -0.488133   -0.499602           1.201692     -0.772085   \n",
      "      ABBV      -0.457299   -0.487787           0.082550     -0.036496   \n",
      "      ABT       -0.443102   -0.508513           0.019517      0.721704   \n",
      "      ACGL      -0.487660   -0.486642           0.340338     -0.767860   \n",
      "\n",
      "              sortino_ratio  max_drawdown    var_95      beta  liquidity  \\\n",
      "Price Ticker                                                               \n",
      "Close A           -0.229715      0.805469  0.505174 -0.469136  -0.115417   \n",
      "      AAPL        -0.741814      0.993460  0.484087 -0.464818  -0.115415   \n",
      "      ABBV        -0.421039      0.700538  0.526798 -0.467043  -0.115417   \n",
      "      ABT          0.235046      1.200175  0.542720 -0.476886  -0.115418   \n",
      "      ACGL        -0.737378      0.310649  0.492774 -0.461966  -0.115422   \n",
      "\n",
      "              hierarchical_cluster  gmm_cluster  dbscan_cluster  \n",
      "Price Ticker                                                     \n",
      "Close A                          3            6               0  \n",
      "      AAPL                       2            8               0  \n",
      "      ABBV                       3            2               0  \n",
      "      ABT                        0            2               0  \n",
      "      ACGL                       3            6               0  \n"
     ]
    }
   ],
   "source": [
    "# from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# ==========================\n",
    "# Step 3: Advanced Clustering (Hierarchical + GMM + DBSCAN)\n",
    "# ==========================\n",
    "# Use scaled features for clustering\n",
    "X_scaled = features_scaled_df.copy()\n",
    "\n",
    "# Hierarchical Clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=10, linkage='ward')\n",
    "X_scaled['hierarchical_cluster'] = hierarchical.fit_predict(features_scaled_df)\n",
    "\n",
    "# Gaussian Mixture Model (GMM)\n",
    "gmm = GaussianMixture(n_components=10, random_state=42)\n",
    "X_scaled['gmm_cluster'] = gmm.fit_predict(features_scaled_df)\n",
    "\n",
    "# DBSCAN Clustering (Density-based)\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
    "X_scaled['dbscan_cluster'] = dbscan.fit_predict(features_scaled_df)\n",
    "\n",
    "# Save Clustering Data\n",
    "X_scaled.to_csv(\"advanced_clustered_data.csv\")\n",
    "\n",
    "# Display first few rows of results\n",
    "print(X_scaled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Step 4: Portfolio Optimization (Mean-Variance)\n",
    "# ==========================\n",
    "\n",
    "def portfolio_performance(weights, returns):\n",
    "    \"\"\"Calculate portfolio return, volatility, and Sharpe ratio.\"\"\"\n",
    "    portfolio_return = np.sum(weights * returns.mean()) * 252\n",
    "    portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    sharpe_ratio = portfolio_return / portfolio_volatility if portfolio_volatility > 0 else 0\n",
    "    return portfolio_return, portfolio_volatility, sharpe_ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_sharpe(weights, returns):\n",
    "    \"\"\"Objective function to minimize negative Sharpe ratio.\"\"\"\n",
    "    return -portfolio_performance(weights, returns)[2]  # Minimize negative Sharpe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_portfolio(returns):\n",
    "    \"\"\"Optimize portfolio allocation using Mean-Variance optimization.\"\"\"\n",
    "    returns = returns.dropna(axis=1)  # Ensure valid returns data\n",
    "\n",
    "    num_assets = len(returns.columns)\n",
    "    initial_weights = np.ones(num_assets) / num_assets\n",
    "    bounds = [(0, 1) for _ in range(num_assets)]  # Weight constraints (0 to 1)\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}  # Sum of weights must be 1\n",
    "\n",
    "    optimized = minimize(minimize_sharpe, initial_weights, args=(returns,), \n",
    "                         method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    return optimized.x if optimized.success else None  # Return weights if optimization is successful\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select stocks from hierarchical clustering (e.g., cluster 0)\n",
    "selected_cluster = 0\n",
    "selected_stocks = X_scaled[X_scaled['hierarchical_cluster'] == selected_cluster].index\n",
    "cluster_returns = returns[selected_stocks]  # Use only clustered stocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimize portfolio for selected cluster\n",
    "optimized_weights = optimize_portfolio(cluster_returns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Portfolio Allocation:\n",
      "Price  Ticker\n",
      "Close  ABT       1.129009e-02\n",
      "       ACN       2.644893e-03\n",
      "       ADP       1.181757e-10\n",
      "       ADSK      0.000000e+00\n",
      "       AEE       2.386078e-02\n",
      "                     ...     \n",
      "Open   WMT       5.613280e-03\n",
      "       WTW       1.902605e-10\n",
      "       WY        0.000000e+00\n",
      "       XEL       4.838965e-03\n",
      "       ZBRA      0.000000e+00\n",
      "Length: 694, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Display optimized portfolio weights\n",
    "portfolio_allocation = pd.Series(optimized_weights, index=cluster_returns.columns)\n",
    "print(\"Optimized Portfolio Allocation:\")\n",
    "print(portfolio_allocation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
